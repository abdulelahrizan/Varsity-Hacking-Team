{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disaster Tweets Shallow Machine Learning Solution",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidAltrows/NLP/blob/master/Disaster_Tweets_Shallow_Machine_Learning_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E88AUop2hJ-",
        "colab_type": "text"
      },
      "source": [
        "##Intro\n",
        "\n",
        "**Small update**: I am currently completing an updated solution through tensorflow which has easy solutions for deep learning in NLP. This includes making full use of tokenization, sequences, and word embeddings! In this notebook however I will be explaining every step made in this binary classification model! I invite you to ask questions or contribute yourself!\n",
        "\n",
        "**My personal to-do** in this project:\n",
        "- **Improove handling of categorical data**\n",
        "- **Create data pipeline**\n",
        "\n",
        "\n",
        "##Problem description\n",
        "\n",
        "Twitter has become an important communication channel in times of emergency.\n",
        "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
        "\n",
        "But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:\n",
        "\n",
        "<div>\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/competitions/tweet_screenshot.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "The data for these tweets can be found here: https://www.figure-eight.com/data-for-everyone/\n",
        "\n",
        "Credit goes to: The Kaggle micro-courses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Evaluation**\n",
        "\n",
        "This problem uses F1 as an evaluation metric. F1 is a measure of accuracy in binary classification, formula specifics can be found here: https://en.wikipedia.org/wiki/F1_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Solution**\n",
        "\n",
        "This is the first notebook solution for this problem:\n",
        "\n",
        "\n",
        "This notebook will be divided into the following parts<br>\n",
        "<ol>\n",
        "    <li><b>Data Exploration</b></li>\n",
        "    <li><b>Data Preprocessing</b></li>\n",
        "    <li><b>Basic NLP Techniques</b></li>\n",
        "    <li><b>Models Bulding</b></li>\n",
        "    <li><b>Models evaluation</b></li>\n",
        "</ol>\n",
        "Importing libraries and loaidng data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgdASrub3dRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "nltk.download('stopwords')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Important libraries loaded successfully\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVaddpt5X9Q",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data Exploration\n",
        "Pandas is an exceptional library for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hu-6fHM5epu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/DavidAltrows/NLP/master/train.csv'\n",
        "data_train = pd.read_csv(url)\n",
        "print(\"Data shape = \",data_train.shape)\n",
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StVpHc2JTD3S",
        "colab_type": "text"
      },
      "source": [
        "Clearly there is some missing values that must be delt with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAAU3bxaTK7o",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data Preprocessing\n",
        "\n",
        "Data Preprocessing one of the most important steps in any data science or machine learning project. In this first version I set up the process but did not invest too much time into this step.\n",
        "## 2.1 Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f131wO4LV1WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get total count of data including missing data\n",
        "total = data_train.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "#get percent of missing data relevant to all data\n",
        "percent = (data_train.isnull().sum()/data_train.isnull().count()).sort_values(ascending=False)\n",
        "\n",
        "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "missing_data.head(data_train.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hki8FiupV_UL",
        "colab_type": "text"
      },
      "source": [
        "As seen in above table ~33% of **location** column is missing and very small percentage of **keyword** column is missing.<br>\n",
        "\n",
        "\n",
        "## 2.2 Handling Missing Data\n",
        "\n",
        "This image is one of my favorite roadmaps to handle **missing data** \n",
        "<img src='https://miro.medium.com/max/1528/1*_RA3mCS30Pr0vUxbp25Yxw.png' width=\"550px\" style='float:left;'>\n",
        "<div style='clear:both'></div>\n",
        "<br>\n",
        "\n",
        "\n",
        "In this model I will use deletion. In a future branch you will likely see me try out possible one-hot encoding for keywords or some form of label encoding. Imputation is not possible for missing categorical values.\n",
        "\n",
        "\n",
        "In **Deletion** I will use **Deleting Columns** technique. Now I will drop **location** and **keyword** columns. As well as the id column as it has no connection to the tweet's sentement.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVef_k0-Xu63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = data_train.drop(['id','location','keyword'], axis=1)\n",
        "print(\"location and keyword columns droped successfully\")\n",
        "#make sure the data only has the target and tweet text\n",
        "data_train.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjdFddJ_Z-Rp",
        "colab_type": "text"
      },
      "source": [
        "# 3. NLP Technique\n",
        "\n",
        "<div style='clear:both'></div>\n",
        "<hr>\n",
        "Before starting text preprocess steps I must define two terms: Corpus and Bag of words (These descriptions are directly from wiki)\n",
        "\n",
        "**Corpus :** Is a large and structured set of texts, We can consider it as simplified version of our text data that contain clean and benefit data.<br>\n",
        "\n",
        "**Bag of words :** In practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \"bag of words\", we can calculate various measures to characterize the text [wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)<br>\n",
        "\n",
        "\n",
        "In this version these are the processing steps that will be applied before the shallow learning models are tested.\n",
        "\n",
        "<ol>\n",
        "    <li><b>Remove unwanted words</b></li>\n",
        "    <li><b>Transform words to lowercase</b></li>\n",
        "    <li><b>Remove stopwords</b></li>\n",
        "    <li><b>Stemming words</b></li>\n",
        "    <li><b>Create sparse matrix ( Bag of words )</b></li>\n",
        "</ol>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-XCa-NBPaiQ",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Remove unwanted words\n",
        "As we see our **text** column contains unwanted words as **#, =>, numbers, or ... etc** these letters will not be useful in our problem so we will get only pure text without any markings or numbers. Further analysis might determine this assumption is incorrect.<br>\n",
        "\n",
        "We will do it by **specifying** our pattern using **re** library. This can be done many other ways.\n",
        "\n",
        "## 3.2 Transform words to lowercase\n",
        "\n",
        "We must transform words to lowercase because each letter has own **ASCII Code** that represent text in computers. Uppercase letter has different ASCII Code than same letter in lowercase format. **so that** 'A' letter differ from 'a' letter in computer. This conversion is a simple method of tokenization.\n",
        "\n",
        "## 3.3 Remove stopwords\n",
        "**Stop words :** are generally the most common words in a language, so we will remove it to prevent misleading problem in our model. The stop words in this model may be redefined if insights are drawn from model failures.\n",
        "\n",
        "## 3.4 Stemming words\n",
        "**stemming :** is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form https://en.wikipedia.org/wiki/Stemming.\n",
        "\n",
        "We use stemming to reduce **bag of words** dimensionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb7l2K6FSeiY",
        "colab_type": "code",
        "outputId": "79fb751a-57d3-4f47-cf7e-db18cdec3be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "corpus  = []\n",
        "pstem = PorterStemmer()\n",
        "for i in range(data_train['text'].shape[0]):\n",
        "    #Remove unwanted words\n",
        "    tweet = re.sub(\"[^a-zA-Z]\", ' ', data_train['text'][i])\n",
        "    #Transform words to lowercase\n",
        "    tweet = tweet.lower()\n",
        "    tweet = tweet.split()\n",
        "    #Remove stopwords then Stemming it\n",
        "    tweet = [pstem.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
        "    tweet = ' '.join(tweet)\n",
        "    #Append cleaned tweet to corpus\n",
        "    corpus.append(tweet)\n",
        "    \n",
        "print(\"Corpus created successfully\")  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus created successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45YmqpzPSgZY",
        "colab_type": "text"
      },
      "source": [
        "**Let's explore corpus, and discover the difference between raw and clean text data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3r-etI_Siwb",
        "colab_type": "code",
        "outputId": "cfc07dc8-e217-4cb4-c9ab-5fd5d1079956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "print(pd.DataFrame(corpus)[0].head(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0            deed reason earthquak may allah forgiv us\n",
            "1                 forest fire near la rong sask canada\n",
            "2    resid ask shelter place notifi offic evacu she...\n",
            "3          peopl receiv wildfir evacu order california\n",
            "4    got sent photo rubi alaska smoke wildfir pour ...\n",
            "5    rockyfir updat california hwi close direct due...\n",
            "6    flood disast heavi rain caus flash flood stree...\n",
            "7                               top hill see fire wood\n",
            "8               emerg evacu happen build across street\n",
            "9                             afraid tornado come area\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp418eftSkiD",
        "colab_type": "code",
        "outputId": "87f3a425-259e-4bd8-e2e2-d697471f2b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "rawTexData = data_train[\"text\"].head(10)\n",
        "cleanTexData = pd.DataFrame(corpus, columns=['text after cleaning']).head(10)\n",
        "\n",
        "frames = [rawTexData, cleanTexData]\n",
        "result = pd.concat(frames, axis=1, sort=False)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text after cleaning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>deed reason earthquak may allah forgiv us</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>forest fire near la rong sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>peopl receiv wildfir evacu order california</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>rockyfir updat california hwi close direct due...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>flood disast heavi rain caus flash flood stree...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>top hill see fire wood</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>emerg evacu happen build across street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>afraid tornado come area</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                text after cleaning\n",
              "0  Our Deeds are the Reason of this #earthquake M...          deed reason earthquak may allah forgiv us\n",
              "1             Forest fire near La Ronge Sask. Canada               forest fire near la rong sask canada\n",
              "2  All residents asked to 'shelter in place' are ...  resid ask shelter place notifi offic evacu she...\n",
              "3  13,000 people receive #wildfires evacuation or...        peopl receiv wildfir evacu order california\n",
              "4  Just got sent this photo from Ruby #Alaska as ...  got sent photo rubi alaska smoke wildfir pour ...\n",
              "5  #RockyFire Update => California Hwy. 20 closed...  rockyfir updat california hwi close direct due...\n",
              "6  #flood #disaster Heavy rain causes flash flood...  flood disast heavi rain caus flash flood stree...\n",
              "7  I'm on top of the hill and I can see a fire in...                             top hill see fire wood\n",
              "8  There's an emergency evacuation happening now ...             emerg evacu happen build across street\n",
              "9  I'm afraid that the tornado is coming to our a...                           afraid tornado come area"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epPxCwtvSmVq",
        "colab_type": "text"
      },
      "source": [
        "As we know that there some words that repeated so little in our tweets, so we must remove these words from our **Bag of words** to decrease dimensionality as possible.<br>\n",
        "\n",
        "We will do it by create dictionary where **key** refer to **word** and **value** refer to **word frequents in all tweets**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQU8s6PSSoRb",
        "colab_type": "code",
        "outputId": "cb028c3e-e150-475a-96df-81b7b39746ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "#Create our dictionary \n",
        "uniqueWordFrequents = {}\n",
        "for tweet in corpus:\n",
        "    for word in tweet.split():\n",
        "        if(word in uniqueWordFrequents.keys()):\n",
        "            uniqueWordFrequents[word] += 1\n",
        "        else:\n",
        "            uniqueWordFrequents[word] = 1\n",
        "            \n",
        "#Convert dictionary to dataFrame\n",
        "uniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\n",
        "uniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\n",
        "uniqueWordFrequents.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word Frequent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>co</th>\n",
              "      <td>4746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>http</th>\n",
              "      <td>4721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fire</th>\n",
              "      <td>363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amp</th>\n",
              "      <td>344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>get</th>\n",
              "      <td>311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bomb</th>\n",
              "      <td>239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>new</th>\n",
              "      <td>228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>via</th>\n",
              "      <td>220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u</th>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Word Frequent\n",
              "co             4746\n",
              "http           4721\n",
              "like            411\n",
              "fire            363\n",
              "amp             344\n",
              "get             311\n",
              "bomb            239\n",
              "new             228\n",
              "via             220\n",
              "u               216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlr994fwSqEZ",
        "colab_type": "code",
        "outputId": "ab4b6169-90c2-4b0d-a8d2-6a641ee8064e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "uniqueWordFrequents['Word Frequent'].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4746, 4721,  411,  363,  344,  311,  239,  228,  220,  216,  213,\n",
              "        210,  209,  201,  183,  181,  180,  178,  175,  169,  166,  164,\n",
              "        162,  156,  155,  153,  151,  145,  144,  143,  137,  133,  132,\n",
              "        131,  130,  129,  128,  125,  124,  123,  122,  121,  120,  119,\n",
              "        118,  117,  116,  114,  111,  110,  109,  108,  106,  105,  104,\n",
              "        103,  102,  101,  100,   99,   98,   97,   96,   95,   94,   93,\n",
              "         91,   90,   89,   88,   87,   86,   84,   83,   82,   79,   78,\n",
              "         77,   76,   75,   74,   73,   72,   71,   70,   69,   68,   67,\n",
              "         66,   65,   64,   63,   62,   61,   60,   59,   58,   57,   56,\n",
              "         55,   54,   53,   52,   51,   50,   49,   48,   47,   46,   45,\n",
              "         44,   43,   42,   41,   40,   39,   38,   37,   36,   35,   34,\n",
              "         33,   32,   31,   30,   29,   28,   27,   26,   25,   24,   23,\n",
              "         22,   21,   20,   19,   18,   17,   16,   15,   14,   13,   12,\n",
              "         11,   10,    9,    8,    7,    6,    5,    4,    3,    2,    1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQO8oSDoSsZ8",
        "colab_type": "text"
      },
      "source": [
        "As we see some words repeated a lot and others repeated less, so we will get only words that repeated more than or equal 20 once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVYHJVTBSuls",
        "colab_type": "code",
        "outputId": "79aa954b-bd38-4e97-8252-ff9b3d648232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\n",
        "print(uniqueWordFrequents.shape)\n",
        "uniqueWordFrequents"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(787, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word Frequent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>co</th>\n",
              "      <td>4746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>http</th>\n",
              "      <td>4721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fire</th>\n",
              "      <td>363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amp</th>\n",
              "      <td>344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cnn</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gem</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>captur</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>arriv</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carri</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>787 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Word Frequent\n",
              "co               4746\n",
              "http             4721\n",
              "like              411\n",
              "fire              363\n",
              "amp               344\n",
              "...               ...\n",
              "cnn                20\n",
              "gem                20\n",
              "captur             20\n",
              "arriv              20\n",
              "carri              20\n",
              "\n",
              "[787 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOJ7NCsMVMJG",
        "colab_type": "text"
      },
      "source": [
        "## 3.5 Create sparse matrix ( Bag of words )\n",
        "**Bag of word** contain only unique words in corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHqz4W3TVOeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\n",
        "bagOfWords = counVec.fit_transform(corpus).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCHw2jszVQOe",
        "colab_type": "text"
      },
      "source": [
        "# 4. Building Models\n",
        "Now we will build our models, we will use following models\n",
        "* Decision Tree Model\n",
        "* Gradient Boosting Model\n",
        "* K - Nearest Neighbors Model\n",
        "* Logistic Regression Model\n",
        "* Support Vector Machine Model\n",
        "* Voting Classifier Model - Decided not to\n",
        "\n",
        "But before using it we will split our data to train and test set first.\n",
        "\n",
        "\n",
        "**Aside**\n",
        "\n",
        "The hyperparameters used in these models are likely not optimal however this project is focused on NLP.\n",
        "\n",
        "If you're interesting learning more about classification models this article is excellent: https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bylaQsehVTjA",
        "colab_type": "code",
        "outputId": "66115755-ee96-44af-b9aa-87d19f065ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "X = bagOfWords\n",
        "y = data_train['target']\n",
        "print(\"X shape = \",X.shape)\n",
        "print(\"y shape = \",y.shape)\n",
        "\n",
        "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.20, random_state=55, shuffle =True)\n",
        "print('data splitting successfully')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape =  (7613, 787)\n",
            "y shape =  (7613,)\n",
            "data splitting successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F8rIIgUVXt6",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naFF3sTMVYUg",
        "colab_type": "code",
        "outputId": "ce305616-06d4-43cf-e891-a333debfaccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',max_depth = None, splitter='best', random_state=55)\n",
        "\n",
        "decisionTreeModel.fit(X_train,y_train)\n",
        "\n",
        "print(\"decision Tree Classifier model run successfully\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decision Tree Classifier model run successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSIruzq1VYg4",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 Gradient Boosting Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkZgEel2VYrn",
        "colab_type": "code",
        "outputId": "2fc3ac4b-ecff-4478-f039-68c9fa078974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gradientBoostingModel = GradientBoostingClassifier(loss = 'deviance',\n",
        "                                                   learning_rate = 0.01,\n",
        "                                                   n_estimators = 100,\n",
        "                                                   max_depth = 30,\n",
        "                                                   random_state=55)\n",
        "\n",
        "gradientBoostingModel.fit(X_train,y_train)\n",
        "\n",
        "print(\"gradient Boosting Classifier model run successfully\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradient Boosting Classifier model run successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fouVmGxVZPY",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 K - Nearest Neighbors Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh-fc0oqVZZp",
        "colab_type": "code",
        "outputId": "fd40876a-7727-4381-e73d-202779e93dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "KNeighborsModel = KNeighborsClassifier(n_neighbors = 7,\n",
        "                                       weights = 'distance',\n",
        "                                      algorithm = 'brute')\n",
        "\n",
        "KNeighborsModel.fit(X_train,y_train)\n",
        "\n",
        "print(\"KNeighbors Classifier model run successfully\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNeighbors Classifier model run successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1tXtw5VVZzv",
        "colab_type": "text"
      },
      "source": [
        "## 4.4 Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKlS53hUVZ_Q",
        "colab_type": "code",
        "outputId": "50eba069-f7a9-42a9-8c55-5db6137d43c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "LogisticRegression = LogisticRegression(penalty='l2', \n",
        "                                        solver='saga', \n",
        "                                        random_state = 55)  \n",
        "\n",
        "LogisticRegression.fit(X_train,y_train)\n",
        "\n",
        "print(\"LogisticRegression Classifier model run successfully\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression Classifier model run successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZDyVIZ7W-yF",
        "colab_type": "text"
      },
      "source": [
        "## 4.5 Support Vector Machine Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlOCUJmIYJ5V",
        "colab_type": "code",
        "outputId": "4f451d17-8463-4b64-d382-ee74bd6c42bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "SVClassifier = SVC(kernel= 'linear',\n",
        "                   degree=3,\n",
        "                   max_iter=10000,\n",
        "                   C=2, \n",
        "                   random_state = 55)\n",
        "\n",
        "SVClassifier.fit(X_train,y_train)\n",
        "\n",
        "print(\"SVClassifier model run successfully\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVClassifier model run successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD_vuwuoXO_P",
        "colab_type": "text"
      },
      "source": [
        "## 4.6 Voting Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6n79ZWIXPiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#modelsNames = []\n",
        "\n",
        "#votingClassifier = VotingClassifier(voting = 'hard',estimators= modelsNames)\n",
        "#votingClassifier.fit(X_train,y_train)\n",
        "#print(\"votingClassifier model run successfully\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY-flj7yXR5C",
        "colab_type": "text"
      },
      "source": [
        "# 5 Models evaluation\n",
        "\n",
        "Now we will evaluate our model using **f1_score** let's go. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlL4MyVoXUNi",
        "colab_type": "code",
        "outputId": "7af504a9-7b78-4b1d-b672-4ef79491b5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "#evaluation Details\n",
        "models = [decisionTreeModel, gradientBoostingModel, KNeighborsModel, LogisticRegression, SVClassifier]\n",
        "\n",
        "for model in models:\n",
        "    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n",
        "    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n",
        "    \n",
        "    y_pred = model.predict(X_test)\n",
        "    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test,y_pred))\n",
        "    print('--------------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier  Train Score is   :  0.9761904761904762\n",
            "DecisionTreeClassifier  Test Score is    :  0.7419566644780039\n",
            "DecisionTreeClassifier  F1 Score is      :  0.6743993371996686\n",
            "--------------------------------------------------------------------------\n",
            "GradientBoostingClassifier  Train Score is   :  0.8594417077175698\n",
            "GradientBoostingClassifier  Test Score is    :  0.7511490479317138\n",
            "GradientBoostingClassifier  F1 Score is      :  0.6331074540174251\n",
            "--------------------------------------------------------------------------\n",
            "KNeighborsClassifier  Train Score is   :  0.9761904761904762\n",
            "KNeighborsClassifier  Test Score is    :  0.7406434668417596\n",
            "KNeighborsClassifier  F1 Score is      :  0.5872518286311389\n",
            "--------------------------------------------------------------------------\n",
            "LogisticRegression  Train Score is   :  0.8502463054187193\n",
            "LogisticRegression  Test Score is    :  0.7826657912015759\n",
            "LogisticRegression  F1 Score is      :  0.7230125523012553\n",
            "--------------------------------------------------------------------------\n",
            "SVC  Train Score is   :  0.8574712643678161\n",
            "SVC  Test Score is    :  0.7741300065659882\n",
            "SVC  F1 Score is      :  0.7180327868852457\n",
            "--------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nspBS-6-aqQW",
        "colab_type": "text"
      },
      "source": [
        "There is overtraining based on the differences in the scores seen here, I'll likely investigate this in the future."
      ]
    }
  ]
}